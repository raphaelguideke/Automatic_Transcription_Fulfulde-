{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bcb563a-c40a-4411-b5b1-14fa371ddf95",
   "metadata": {},
   "source": [
    "# 1. Prerequisites and Installation\n",
    "\n",
    "To begin, you need to **install the necessary Python libraries**.\n",
    "\n",
    "We will use the **Hugging Face libraries**, which greatly simplify the training of state-of-the-art models.\n",
    "\n",
    "* **transformers**: To access pre-trained models like **Whisper**.\n",
    "* **datasets**: To manage and preprocess your audio-text database.\n",
    "* **accelerate**: For easy management of GPU training.\n",
    "* **soundfile** and **librosa**: For processing audio files.\n",
    "* **jiwer**: To evaluate the **Word Error Rate (WER)**.\n",
    "* **gradio**: To create the graphical interface.\n",
    "* **torch**: The machine learning framework (or tensorflow if you prefer).\n",
    "\n",
    "The best approach is to **fine-tune a pre-trained model** on a large multilingual corpus, such as **OpenAI's Whisper**, on our own Fulfulde database. Here is a complete guide to achieve this using a Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d922e17b-7d61-44ab-8c3d-2254b4f89282",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers datasets accelerate soundfile librosa jiwer gradio torch\n",
    "!pip install torchaudio torchcodec\n",
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2029a1-72e5-4d18-8110-eab044c71c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be placed in the VERY FIRST cell of our notebook\n",
    "!pip install --upgrade transformers accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6d3530-ff60-4271-ac1e-279794558271",
   "metadata": {},
   "source": [
    "# 2. Database Structure\n",
    "For fine-tuning, our database must have a specific structure. We need audio-text pairs.\n",
    "- File type: Audio files can be in .mp3 format. (Example: ful1.mp3; ful2.mp3; ful3.mp3; ful4.mp3; ful5.mp3)\n",
    "- Text files must be simple .txt files. (Example: ful1.txt; ful2.txt; ful3.txt; ful4.txt; ful5.txt)\n",
    "- Our system scans a local directory, associates the mp3 audio files with their corresponding txt text files, and creates the DataFrame for training. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad177615-9ee5-4b3c-928d-ea4d0278980f",
   "metadata": {},
   "source": [
    "# 3. Python code for fine-tuning in a Jupyter Notebook\n",
    "This code covers data loading, fine-tuning the Whisper model, and evaluating its performance.\n",
    "\n",
    "### A. Loading data from a local directory\n",
    "1. Defining the directory: We must first define the path to the folder where our audio and text files are located.\n",
    "2. Browsing audio files: The glob.glob( ) function is used to find all files with the .mp3 extension in the specified directory.\n",
    "3. Pairing: For each audio file found, the code constructs the expected transcript file name by replacing the .mp3 extension with .txt (os.path.splitext()).\n",
    "4. Checking for existence: It then checks whether this text file exists. If it does not, a warning message is displayed and the audio file is ignored, thus avoiding missing transcription errors.\n",
    "5. Reading transcripts: If the .txt file is found, its contents are read and stored as the transcript. Utf-8 encoding is specified to correctly handle Fulfulde special characters.\n",
    "6. Creating the DataFrame: Finally, the paths to the audio files and transcripts are combined into a dictionary, which is used to create the DataFrame df needed for the next step of fine-tuning.\n",
    "\n",
    "This solution is more flexible because it automatically generates the DataFrame based on the files we place in our working directory, which simplifies database management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518c8ea9-db0c-4748-bda6-20c708f3ec1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from datasets import Dataset, Audio\n",
    "\n",
    "data_dir = 'C:/Users/CCI-CNDT/tpt/transcription/data'\n",
    "if not os.path.isdir(data_dir):\n",
    "    raise FileNotFoundError(f\"Error: The directory '{data_dir}' does not exist. Please create it and place your files there.\")\n",
    "\n",
    "audio_paths = []Browse all files in the directory to find audio-text pairs\n",
    "for filepath in glob.glob(os.path.join(data_dir, '*.mp3')):\n",
    "    transcript_path = os.path.splitext(filepath)[0] + '.txt'\n",
    "    \n",
    "    if os.path.exists(transcript_path):\n",
    "        try:\n",
    "            with open(transcript_path, 'r', encoding='utf-8') as f:\n",
    "                transcript_text = f.read().strip()\n",
    "            \n",
    "            audio_paths.append(filepath)\n",
    "            transcriptions.append(transcript_text)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {transcript_path} : {e}\")\n",
    "    else:\n",
    "        print(f\"Warning: Missing transcript file for {filepath}. This file will be ignored.\")\n",
    "\n",
    "if not audio_paths:\n",
    "    raise ValueError(\"No valid audio/text pairs (mp3 + txt) were found in the directory. Ensure that each .mp3 file has a corresponding .txt file with the same name.)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"path\": audio_paths,\n",
    "    \"transcription\": transcriptions\n",
    "})\n",
    "\n",
    "print(f\"\\nDataFrame successfully created. {len(df)} audio-text pairs found. Here are the first 5 lines :\\n\")\n",
    "print(df.head())\n",
    "\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "dataset = dataset.cast_column(\"path\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c5f05b-90a2-4ace-9685-19e7129a18db",
   "metadata": {},
   "source": [
    "### B. Loading and Preprocessing with Hugging Face "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2ecf5e-6ddb-40dd-9dfb-9ac0b678e53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperFeatureExtractor, WhisperTokenizer, WhisperProcessor\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-tiny\")\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-tiny\", task=\"transcribe\")\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\", task=\"transcribe\")\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    \"\"\"\n",
    "    Pre-processing function for the database.\n",
    "    \"\"\"\n",
    "    audio = batch[\"path\"]\n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "    batch[\"labels\"] = tokenizer(batch[\"transcription\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "tokenized_dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names)\n",
    "\n",
    "filtered_dataset = tokenized_dataset.filter(\n",
    "    lambda example: len(example[\"labels\"]) < 1024,\n",
    "    num_proc=1 \n",
    ")\n",
    "\n",
    "print(\"\\n--- Data filtering ---\")\n",
    "print(f\"Number of samples before filtering : {len(tokenized_dataset)}\")\n",
    "print(f\"Number of samples after filtering : {len(filtered_dataset)}\")\n",
    "print(f\"Number of samples deleted : {len(tokenized_dataset) - len(filtered_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052e379b-446f-4704-8045-8ef1bb177b83",
   "metadata": {},
   "source": [
    "### C. Model Fine-Tuning\n",
    " It manages the training and saving of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74108d3-e15e-4a79-9922-bb72bbe844a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().item():\n",
    "            labels = labels[:, 1:]\n",
    "        \n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")\n",
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-fulfulde-model\",\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=1,  \n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=50,\n",
    "    max_steps=400,\n",
    "    gradient_checkpointing=False,  \n",
    "    fp16=True,\n",
    "    \n",
    "    eval_strategy=\"steps\",  \n",
    "    \n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    logging_steps=25,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    ")\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    from jiwer import wer\n",
    "    wer_score = wer(label_str, pred_str)\n",
    "    return {\"wer\": wer_score}\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=filtered_dataset, \n",
    "    eval_dataset=filtered_dataset,  \n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(\"./final-model\")\n",
    "processor.save_pretrained(\"./final-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a74d7f-d560-4535-9cb1-64fff70d1df0",
   "metadata": {},
   "source": [
    "## 4. Transcription and Graphical Interface\n",
    "Once the model has been trained, we can use it for transcription via a simple interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1063b0f-abad-4a0f-a0ef-e9631d2f5d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import gradio as gr\n",
    "\n",
    "print(\"Loading the ASR model...\")\n",
    "asr_pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=\"./final-model\",\n",
    "    device=0 if torch.cuda.is_available() else -1,\n",
    ")\n",
    "print(\"Template loaded successfully.\")\n",
    "\n",
    "def transcribe_audio_file(audio_file):\n",
    "    \"\"\"\n",
    "    Function to transcribe an audio file\n",
    "    \"\"\"\n",
    "    if audio_file is None:\n",
    "        return \"Error: Please provide an audio file or record from the microphone..\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"Transcription of the file : {audio_file}\")\n",
    "        transcription = asr_pipe(audio_file)\n",
    "        print(\"Transcription completed.\")\n",
    "        return transcription[\"text\"]\n",
    "    except Exception as e:\n",
    "        print(f\"An error has occurred : {e}\")\n",
    "        return f\"Transcription error : {e}\"\n",
    "\n",
    "def clear_all():\n",
    "    return [None, None]\n",
    "\n",
    "app_css = \"\"\"\n",
    ".gradio-container {\n",
    "    border: 3px solid #1E90FF !important; /* Couleur bleu (dodgerblue) */\n",
    "    border-radius: 15px !important;      /* Coins arrondis */\n",
    "    padding: 15px !important;            /* Espace entre la bordure et le contenu */\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "with gr.Blocks(css=app_css, theme=gr.themes.Default()) as iface:\n",
    "    gr.Markdown(\"# Fulfulde Transcription Service\")\n",
    "    gr.Markdown(\"Download an audio file in Fulfulde or record your voice to obtain the transcription.\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        audio_input = gr.Audio(type=\"filepath\", label=\"Fichier audio Fulfulde\")\n",
    "        \n",
    "        text_output = gr.Textbox(label=\"Transcription\", lines=7) \n",
    "\n",
    "    with gr.Row():\n",
    "        submit_button = gr.Button(\"Transcribe\", variant=\"primary\")\n",
    "        clear_button = gr.Button(\"Clear\")\n",
    "\n",
    "    submit_button.click(\n",
    "        fn=transcribe_audio_file,\n",
    "        inputs=audio_input,\n",
    "        outputs=text_output\n",
    "    )\n",
    "    clear_button.click(\n",
    "        fn=clear_all,\n",
    "        inputs=None,\n",
    "        outputs=[audio_input, text_output]\n",
    "    )\n",
    "\n",
    "print(\"Launch of the Gradio interface...\")\n",
    "iface.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b7c8b8-f16f-420e-8d98-fbc3c306b17a",
   "metadata": {},
   "source": [
    "## 5. Final Evaluation\n",
    "To evaluate the quality of the transcription, the Word Error Rate (WER) is the best metric.\n",
    "It allows us to evaluate the performance of our model once it has been trained, by comparing the generated transcriptions with our reference transcriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2364aa4e-f3a2-4fc3-9086-c87deaaa39eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jiwer\n",
    "\n",
    "def calculate_wer(reference_text, hypothesis_text):\n",
    "    \"\"\"\n",
    "    Calculate the Word Error Rate (WER).\n",
    "    \"\"\"\n",
    "    reference = reference_text.lower().split()\n",
    "    hypothesis = hypothesis_text.lower().split()\n",
    "\n",
    "    error = jiwer.wer(reference, hypothesis)\n",
    "    return error * 100\n",
    "\n",
    "# Example\n",
    "reference = \"Mi yeyi yeeygo ndabbawaaji.\" \n",
    "hypothesis = \"mi yeyi yeeygo ndabbawaaji.\"\n",
    "\n",
    "wer_score = calculate_wer(reference, hypothesis)\n",
    "\n",
    "print(f\"R√©f√©rence : '{reference}'\")\n",
    "print(f\"Transcription du mod√®le : '{hypothesis}'\")\n",
    "print(f\"Score WER : {wer_score:.2f}%\")\n",
    "\n",
    "if wer_score < 10.0:\n",
    "    print(\"The model performs very well. üí™\")\n",
    "elif wer_score < 30.0:\n",
    "    print(\"The model performs acceptably, but could be improved. üìà\")\n",
    "else:\n",
    "    print(\"The model has a high WER; more data or better training is required. üìâ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6651c2f6-5d54-4ceb-a69b-947ad7d42fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import gradio as gr\n",
    "\n",
    "asr_pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=\"./final-model\",\n",
    "    device=0 if torch.cuda.is_available() else -1,\n",
    ")\n",
    "\n",
    "def transcribe_audio_file(audio_file):\n",
    "    \"\"\"\n",
    "    Function to transcribe an audio file\n",
    "    \"\"\"\n",
    "    if audio_file is None:\n",
    "        return \"Error: Please provide an audio file or record from the microphone.\"\n",
    "    \n",
    "    try:\n",
    "        transcription = asr_pipe(audio_file)\n",
    "        return transcription[\"text\"]\n",
    "    except Exception as e:\n",
    "        return f\"Erreur de transcription : {e}\"\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=transcribe_audio_file,\n",
    "    inputs=gr.Audio(type=\"filepath\", label=\"Fulfulde audio file\"),\n",
    "    outputs=gr.Textbox(label=\"Transcription\"),\n",
    "    title=\"Fulfulde Transcription Service\",\n",
    "    description=\"Download an audio file in Fulfulde to obtain its transcription.\"\n",
    ")\n",
    "\n",
    "iface.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
